{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXAve7azp2Qy"
   },
   "source": [
    "# APO Lab 6 Analiza obrazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9v3DebgjLgI"
   },
   "outputs": [],
   "source": [
    "#from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7AthW3AVYw4"
   },
   "source": [
    "!WAŻNE! Aby poniższy plik działał na Google Colab, należy wgrać plik '3shapes.bmp' do Zad1 oraz pliki 'svm_utils.py', 'train_ryz.jpg', 'train_fasola.jpg', 'train_soczewica.jpg', 'test1.jpg' do Zad2. Z panelu po lewej stronie wybieramy ikonę folderu ('Files') a następnie 'Upload' i wybieramy plik (wcześniej ściągnięte na dysk twardy z UBI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhiwTnFcLXMl"
   },
   "source": [
    "# Zadanie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "OfYoPimSjPaX",
    "outputId": "f4bebee4-f235-45f1-a069-50ab59bbe394"
   },
   "outputs": [],
   "source": [
    "# Wczytanie obrazu pierwotnego\n",
    "img = cv2.imread('3shapes.bmp', cv2.IMREAD_GRAYSCALE)\n",
    "#cv2_imshow(img)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L7oo6l5FRbT"
   },
   "source": [
    "Funkcja cv2.findContours pozwala w prosty sposób wybrać z obrazu binarnego obiekty a właściwie ich kontury/obrysy. Jej ierwszym argumentem wejściowym jest oczywiście obraz. Dwa kolejne parametry definiują szczegóły generowania konturów. Drugi parametr określa czy znalezione zostaną wyłącznie kontóry zewnętrzne czy wszystie i określa hierarchiczność. W tym przykładzie będziemy korzystać z opcji *cv2.RETR_LIST*, która generuje wszystkie kontóry w obrazie bez ustalania ich hierarchii. Trzeci paramer określa czy i w jaki sposób kontury obiektów będą przybliżane (aproksymowane) aby zminimalizować liczbę punktów niezbędnych do przechowania. W tym przykładzie będziemy korzystać z opcji *cv2.CHAIN_APPROX_SIMPLE*, która enkoduje linie poziome, pionowe i nachylone pod kątem 45 stopni.\n",
    "\n",
    "Pozostałe opcje drugiego parametru:\n",
    "\n",
    "\n",
    "*   RETR_EXTERNAL \n",
    "Python: cv.RETR_EXTERNAL\n",
    "retrieves only the extreme outer contours. It sets hierarchy[i][2]=hierarchy[i][3]=-1 for all the contours.\n",
    "\n",
    "*   RETR_LIST \n",
    "Python: cv.RETR_LIST\n",
    "retrieves all of the contours without establishing any hierarchical relationships.\n",
    "\n",
    "*   RETR_CCOMP \n",
    "Python: cv.RETR_CCOMP\n",
    "retrieves all of the contours and organizes them into a two-level hierarchy. At the top level, there are external boundaries of the components. At the second level, there are boundaries of the holes. If there is another contour inside a hole of a connected component, it is still put at the top level.\n",
    "\n",
    "*   RETR_TREE \n",
    "Python: cv.RETR_TREE\n",
    "retrieves all of the contours and reconstructs a full hierarchy of nested contours.\n",
    "\n",
    "*   RETR_FLOODFILL \n",
    "Python: cv.RETR_FLOODFILL\n",
    "\n",
    "Pozostałe opcje trzeciego parametru:\n",
    "\n",
    "*   CHAIN_APPROX_NONE \n",
    "Python: cv.CHAIN_APPROX_NONE\n",
    "stores absolutely all the contour points. That is, any 2 subsequent points (x1,y1) and (x2,y2) of the contour will be either horizontal, vertical or diagonal neighbors, that is, max(abs(x1-x2),abs(y2-y1))==1.\n",
    "\n",
    "*   CHAIN_APPROX_SIMPLE \n",
    "Python: cv.CHAIN_APPROX_SIMPLE\n",
    "compresses horizontal, vertical, and diagonal segments and leaves only their end points. For example, an up-right rectangular contour is encoded with 4 points.\n",
    "\n",
    "*   CHAIN_APPROX_TC89_L1 \n",
    "Python: cv.CHAIN_APPROX_TC89_L1\n",
    "applies one of the flavors of the Teh-Chin chain approximation algorithm\n",
    "\n",
    "*   CHAIN_APPROX_TC89_KCOS \n",
    "Python: cv.CHAIN_APPROX_TC89_KCOS\n",
    "applies one of the flavors of the Teh-Chin chain approximation algorithm\n",
    "\n",
    "\n",
    "[Referencja] Dokumentacja: https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#findcontours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pG6LPX1BWezq",
    "outputId": "cb405d76-5fec-4109-b6b9-b78688374db9"
   },
   "outputs": [],
   "source": [
    "# preporcessing\n",
    "ret,thresh = cv2.threshold(img,127,255,0) # pogowanie obrazu pierwotnego aby uzyskac obraz binarny\n",
    "contours,hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE) # funkcja znajdywania kontórow w obrazie binarnym\n",
    "len(contours) # liczb znalezionych konturów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siKhgP2OJntX"
   },
   "source": [
    "Jak łatwo zauważyć pomimo że na obrazie są trzy obiekty to algorytm znalazł 4 kontury. Zobaczmy dlaczego. Narysyjemy wszystkie znalezione kontury na obrazie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "f0LnCCrSiFHx",
    "outputId": "d1bf0dd4-65b0-4122-a25c-f60ff204fe4a"
   },
   "outputs": [],
   "source": [
    "# Rysowanie kontórów na obrazie\n",
    "\n",
    "img2 = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB) # konwersja z szaroodcieniowego do RGB (właściwie to kolejność BGR) aby można było rysować kolorowe kontury\n",
    "\n",
    "# rysujemy pierwszy kontur kolorem niebieskim\n",
    "cnt = contours[0]\n",
    "cv2.drawContours(img2, [cnt], 0, (255,0,0), 3)\n",
    "\n",
    "# rysujemy drugi kontur kolorem zielonym\n",
    "cnt = contours[1]\n",
    "cv2.drawContours(img2, [cnt], 0, (0,255,0), 3)\n",
    "\n",
    "# rysujemy trzeci kontur kolorem czerwonym\n",
    "cnt = contours[2]\n",
    "cv2.drawContours(img2, [cnt], 0, (0,0,255), 3)\n",
    "\n",
    "# rysujemy czwarty kontur innym kolorem\n",
    "cnt = contours[3]\n",
    "cv2.drawContours(img2, [cnt], 0, (255,255,0), 3)\n",
    "\n",
    "#cv2_imshow(img2)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNt-W6joKQJ5"
   },
   "source": [
    "Jak widać algorytm znajduje również kontur najbardziej zewnętrzny, czyli ramkę obrazu. Poniżej definicja funkcji która wczytuje obraz i wyświetla naniesione kolorowe kontury na obraz po progowaniu. Zachęcam do przetestowania różnych kombinacji parametrów funkcji *cv2.findContours*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roL0iwzyjEB_"
   },
   "outputs": [],
   "source": [
    "def find_and_show_contours (fname):\n",
    "  img = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n",
    "  ret,thresh = cv2.threshold(img,127,255,0) \n",
    "  contours,hierarchy = cv2.findContours(thresh, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE) \n",
    "# mode: cv2.RETR_EXTERNAL / cv2.RETR_FLOODFILL\n",
    "# approximation: cv2.CHAIN_APPROX_NONE / cv2.CHAIN_APPROX_SIMPLE\n",
    "  img3 = cv2.cvtColor(thresh,cv2.COLOR_GRAY2RGB)\n",
    "  for cnt in contours:\n",
    "    cv2.drawContours(img3, [cnt], 0, (random.randrange(50,200,25),random.randrange(50,200,25),random.randrange(50,200,25)), 3)\n",
    "  #cv2_imshow(img3)\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.imshow(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "ZfjXWjQZLzsA",
    "outputId": "10164af5-84bd-4c3d-9f15-8cfa644c3e21"
   },
   "outputs": [],
   "source": [
    "find_and_show_contours ('train_fasola.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyLze_e9LPM5"
   },
   "source": [
    "## Zad 1. a) Obliczenie momentów\n",
    "\n",
    "Do obliczania momentów centralnych obrazu (obiektu) służy wbudowana funkcja opencv *cv2.moments*. Może ona przyjmować jako argument wejściowy cały obraz lub kontur (uzyskany z poprzednio użytej funkcji *cv2.findContours*).\n",
    "\n",
    "Funkcja ta wylicza momenty do 3 rzędu wiec w wyniku otrzymujemy 24 wartości. W języku python wynik jest w postaci 'dict'.\n",
    "\n",
    "[Referencja] Dokumentacja: https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=cv2.moments#cv2.moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TJRtu7TxUuCG",
    "outputId": "16b6b4b3-818f-4f7b-e32b-08d27859b347"
   },
   "outputs": [],
   "source": [
    "# obliczanie momentów dla całego obrazu\n",
    "M_img = cv2.moments(img)\n",
    "M_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hr6vZ9nQXBt1",
    "outputId": "68fcc0f7-3c55-4ad7-f8e9-3e062ba0f06d"
   },
   "outputs": [],
   "source": [
    "# obliczanie momentów dla pojedynczego obiektu\n",
    "cnt = contours[0]\n",
    "M = cv2.moments(cnt)\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbKaaK_aLjDY"
   },
   "source": [
    "## Zad 1. b) Obliczenie pola powierzchni i obwodu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIy6ogBIPgAI"
   },
   "source": [
    "Aby obliczyć pole powierzchni i obwodu również można użyć wbudowanych funkcji biblioteki OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKmwB_xrGi7R",
    "outputId": "5a787e09-ed86-4c6b-acce-ca962ad192ca"
   },
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAZMBnw9X6wB",
    "outputId": "5f25a993-5ddf-4fc2-f78e-71a8302737a4"
   },
   "outputs": [],
   "source": [
    "perimeter = cv2.arcLength(cnt,True) # Drugi argument określa czy kształt ma zamknięty kontur (wtedy podajemy True) czy jest to krzywa\n",
    "perimeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6W6QgZenwVp"
   },
   "source": [
    "## Zad 1. c) Obliczanie współczynników kształtu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSxkRa4cQdG-"
   },
   "source": [
    "Do obliczenia wybranych współczynników kształtu również można wspomóc się funkcjami z biblioteki OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cGzFlRsIbeLi",
    "outputId": "015d2dfa-6c47-459d-8a80-80d13e78ab71"
   },
   "outputs": [],
   "source": [
    "#aspect ratio\n",
    "x,y,w,h = cv2.boundingRect(cnt)\n",
    "aspect_ratio = float(w)/h\n",
    "aspect_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpMg8ol1ZUkv",
    "outputId": "3d219a88-2f30-4b5c-eec0-4be15c98a51b"
   },
   "outputs": [],
   "source": [
    "#extent\n",
    "area = cv2.contourArea(cnt)\n",
    "x,y,w,h = cv2.boundingRect(cnt)\n",
    "rect_area = w*h\n",
    "extent = float(area)/rect_area\n",
    "extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avEm5sLyZW_r",
    "outputId": "279470bb-b4eb-4f0f-adc5-ae7d041aef94"
   },
   "outputs": [],
   "source": [
    "#solidity\n",
    "area = cv2.contourArea(cnt)\n",
    "hull = cv2.convexHull(cnt)\n",
    "hull_area = cv2.contourArea(hull)\n",
    "solidity = float(area)/hull_area\n",
    "solidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVS5-A1bZW0A",
    "outputId": "e34cd111-404f-4879-952d-80d1a01f6c9a"
   },
   "outputs": [],
   "source": [
    "# equivalentDiameter \n",
    "area = cv2.contourArea(cnt)\n",
    "equi_diameter = np.sqrt(4*area/np.pi)\n",
    "equi_diameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6nW3jPWb7S6"
   },
   "source": [
    "# Zadanie 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sddAQJEZv4i"
   },
   "source": [
    "Opracować algorytm i uruchomić aplikację realizującą klasyfikację obiektu/obrazu na podstawie utworzonego w Zadaniu 1 wektora cech. Klasyfikacja realizowana za pomocą SVM na podstawie zbioru uczącego podanego przez prowadzącego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiuAvOYkeItF"
   },
   "source": [
    "## Przykład podziału płaszczyzny przez SVM (Support Vector Machine)\n",
    "\n",
    "Zakładając że mamy dwie cechy opisujące każdy obiekt, możemy utworzyć płaszczyznę i nanieść na nią punkty odpowiadające każdemu obiektowi. Załóżmy że wiemy że obiekty należą do dwóch klas. Zadaniem klasyfikatora jest wyznaczenie podziału płaszczyzny na dwa rozłączne obszary odpowiadające każdej z klas, czyli na znalezieniu tzw. granicy decyzyjnej między klasami. Prosty przykład poniżej (nie trzeba wnikliwie analizować kodu, tym zajmiemy się w dalszej części).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PewdQpJaVZDB"
   },
   "outputs": [],
   "source": [
    "# Set up training data\n",
    "labels = np.array([1, -1, -1, -1])\n",
    "trainingData = np.matrix([[501, 10], [255, 10], [501, 255], [10, 501]], dtype=np.float32)\n",
    "#labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1N-DlzoVaCL",
    "outputId": "c7475dbd-8595-47e7-f72a-33904aa1e009"
   },
   "outputs": [],
   "source": [
    "# Train the SVM\n",
    "svm = cv2.ml.SVM_create()\n",
    "svm.setType(cv2.ml.SVM_C_SVC)\n",
    "svm.setKernel(cv2.ml.SVM_LINEAR)\n",
    "svm.setTermCriteria((cv2.TERM_CRITERIA_MAX_ITER, 100, 1e-6))\n",
    "svm.train(trainingData, cv2.ml.ROW_SAMPLE, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "-90e2rLCWH6E",
    "outputId": "4bb222af-d157-4059-a77c-50e283e85005"
   },
   "outputs": [],
   "source": [
    "# Image for visual representation\n",
    "width = 512\n",
    "height = 512\n",
    "image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "# Show the decision regions given by the SVM\n",
    "green = (0,255,0)\n",
    "blue = (255,0,0)\n",
    "for i in range(image.shape[0]):\n",
    "    for j in range(image.shape[1]):\n",
    "        sampleMat = np.matrix([[j,i]], dtype=np.float32)\n",
    "        response = svm.predict(sampleMat)[1]\n",
    "        if response == 1:\n",
    "            image[i,j] = green\n",
    "        elif response == -1:\n",
    "            image[i,j] = blue\n",
    "# Show the training data\n",
    "thickness = -1\n",
    "for dataPoint in trainingData:\n",
    "  cv2.circle(image, (int(dataPoint[0,0]), int(dataPoint[0,1])), 5, (  0,   0,   0), thickness)\n",
    "# Show support vectors\n",
    "thickness = 2\n",
    "sv = svm.getUncompressedSupportVectors()\n",
    "for i in range(sv.shape[0]):\n",
    "    cv2.circle(image, (int(sv[i,0]), int(sv[i,1])), 6, (128, 128, 128), thickness)\n",
    "\n",
    "#cv2_imshow(image)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtfeAB_cTuut"
   },
   "source": [
    "Jak wydać płaszczyna cech została podzielona na dwa zbiory odpowiadające klasom. Linę podziału można by poprowadzić w wielu miejscach. Dlaczego tutaj jest akurat w tym miejscu? \n",
    "Otóż cechą SVM jest wyznaczanie granicy decyzyjnej rozdzielającej z maksymalnym marginesem przykłady należące do różnych klas.\n",
    "\n",
    "![alt text](https://docs.opencv.org/2.4/_images/optimal-hyperplane.png)\n",
    "\n",
    "[Rysunek z: https://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html gdzie można również dowiedzieć się więcej o działaniu SVM]\n",
    "\n",
    "W tych prostych przykładach widocznych na dwóch rysunkach powyżej mamy doczynienia z dwuwymiarową przestrzenią cech. W praktyce dotyczącej klasyfikacji obrazów/obiektów często wektory cech zawierają znacznie więcej niż dwie cechy. Przestrzeń cech staje się wielowymiarowa a granica decyzyjna może być skomplikowaną hiperpłaszczyzną. \n",
    "\n",
    "SVM cechuje się, tym że próbuje oddzielić dane funkcją liniową, a jeśli to nie wychodzi przenosi dane do wyższych wymiarów przy użyciu tzw. funkcji jądrowych (kernel functions) i wtedy jeszcze raz próbuje znaleźć optymalny podział. [tutaj bardzo proste wyjaśnienie działania SVM: https://tomaszkacmajor.pl/index.php/2016/04/17/support-vector-machine/]\n",
    "\n",
    "\n",
    "SVM wciąż jest często wykorzystywany przy klasyfikacji obrazów/obiektów ze względu na dużą efektywność obliczeniową (złożoność rośnie tylko liniowo wraz z liczbą wymiarów), relatywnie szybki czas uczenia klasyfikatora i małe wymagania liczby przykładów zbioru uczącego (szczególnie w porównaniu z popularnym głębokim uczeniem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXVy111ZgOcJ"
   },
   "source": [
    "## Trening i testowanie SVM\n",
    "\n",
    "Klasyfikacja przy użyciu SVM również może być w prosty sposób zrealizowana przy użyciu biblioteki OpenCV. Poniżej przedstawiam przykład klasyfikacji obiektów należących do trzech klas 'ryż', 'soczewica' i 'fasola'. Obiekty będą klasyfikowane na podstawie wektora cech złożonego z cech prezentowanych w Zadaniu 1.\n",
    "\n",
    "Jak większość klasyfikatorów SVM do wyuczenia potrzebuje etykietowanych danych treningowych czyli etykiety w postaci nazwy klasy odpowiedno dla każdego wektora cech.\n",
    "\n",
    "Implementacja SVM w OpenCV wymaga aby dane uczące dla klasyfikatora były podane w postaci macierzy (n,m) gdzie n to liczba przykładów (próbek). Wymiar macierzy (zmiennej) zawierającej etykiety powienien być (n,1). W wersji w języku python zmienne te powinny być typu np.int64.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "id": "hY2X3IjVe21M",
    "outputId": "514afb65-d3a8-411f-9096-d9190c810018"
   },
   "outputs": [],
   "source": [
    "# przygotowane obrazy zawierające dane obiekty zbioru uczącego\n",
    "img1 = cv2.imread('train_ryz.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('train_soczewica.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img3 = cv2.imread('train_fasola.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "#frame = cv2.hconcat((img1, img2))\n",
    "#cv2_imshow(img1)\n",
    "#cv2_imshow(img2)\n",
    "#cv2_imshow(img3)\n",
    "plt.figure()\n",
    "plt.imshow(img1, cmap='gray')\n",
    "plt.figure()\n",
    "plt.imshow(img2, cmap='gray')\n",
    "plt.figure()\n",
    "plt.imshow(img3, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMlUChjMi_Q5"
   },
   "outputs": [],
   "source": [
    "# funkcja tworząca wektor cech dla każdego obiektu zanlezionego w obrazie\n",
    "\n",
    "def get_feat(img):\n",
    "  ret,thresh = cv2.threshold(img,127,255,0)\n",
    "  contours,hierarchy = cv2.findContours(thresh, 1, 2)\n",
    "  #print(len(contours))\n",
    "  #cv2_imshow(thresh)\n",
    "\n",
    "\n",
    "  M_all = np.empty((29, 0))\n",
    "\n",
    "  for cnt in contours:\n",
    "    M = cv2.moments(cnt)\n",
    "    M_vec = M.values()\n",
    "    M_vec = np.array(list(M_vec)).flatten().reshape(-1,1)\n",
    "    #print(M_vec.shape)\n",
    "\n",
    "    area = cv2.contourArea(cnt)\n",
    "\n",
    "    perimeter = cv2.arcLength(cnt,True)\n",
    "\n",
    "    x,y,w,h = cv2.boundingRect(cnt)\n",
    "    aspect_ratio = float(w)/h\n",
    "\n",
    "    rect_area = w*h\n",
    "    extent = float(area)/rect_area\n",
    "\n",
    "    equi_diameter = np.sqrt(4*area/np.pi)\n",
    "\n",
    "    feat_vec = np.array([area, perimeter, aspect_ratio, extent, equi_diameter]).reshape(-1,1)\n",
    "\n",
    "    M_vec = np.vstack((M_vec, feat_vec))\n",
    "    #print(M_vec.shape)\n",
    "    M_all = np.hstack((M_all, M_vec))\n",
    "\n",
    "  return M_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCJP0wPwjOdl"
   },
   "outputs": [],
   "source": [
    "# funkcja tworząca wektor zadanych etykiet odpowadający rozmiarem wektorowi cech\n",
    "\n",
    "def get_target(input_feats, target_class = 1):\n",
    "  sh = input_feats.shape\n",
    "  out = np.ones((sh[1],1))\n",
    "  return out*target_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goSULoMKjOa8",
    "outputId": "5d8ae379-67b3-4b12-c368-fba83d14fefc"
   },
   "outputs": [],
   "source": [
    "# dla każdego obiektu ze zbioru uczącego generujemy wektor cech\n",
    "\n",
    "img2 = cv2.imread('train_ryz.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "feat1 = get_feat(img2)\n",
    "feat1 = np.delete(feat1, feat1.shape[1]-1, axis=1) # usuwamy wektor cech konturu odpowiadającego ramce obrazu\n",
    "\n",
    "img2 = cv2.imread('train_soczewica.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "feat2 = get_feat(img2)\n",
    "feat2 = np.delete(feat2, feat2.shape[1]-1, axis=1)\n",
    "\n",
    "img2 = cv2.imread('train_fasola.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "feat3 = get_feat(img2)\n",
    "feat3 = np.delete(feat3, feat3.shape[1]-1, axis=1)\n",
    "\n",
    "# konkatencja czyli łączenie wektorów cech w jedną macierz\n",
    "x_input = np.float32(np.concatenate((feat1,np.concatenate((feat2, feat3),axis =1)),axis =1).transpose())\n",
    "\n",
    "print('Kształt macierzy cech: ', x_input.shape)\n",
    "print('Gdzie mamy ', x_input.shape[1],' cech oraz ', x_input.shape[0],' przykładów.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyfhKSShkURl",
    "outputId": "32d56ecf-d7b1-4fa4-a23b-99523f9a501a"
   },
   "outputs": [],
   "source": [
    "# przygotowanie zbiorczego wektora etykiet\n",
    "t1 = get_target(feat1, 1)\n",
    "t2 = get_target(feat2, 2)\n",
    "t3 = get_target(feat3, 3)\n",
    "t_input = np.int64(np.concatenate((t1,np.concatenate((t2, t3)))))\n",
    "\n",
    "print('Kształt wektora etykiet: ', t_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4Parii0i_Mi",
    "outputId": "3240aecf-7f03-4128-d1bd-4bb727a267de"
   },
   "outputs": [],
   "source": [
    "# Train the SVM\n",
    "mysvm = cv2.ml.SVM_create() # utowrzenie SVM\n",
    "mysvm.setType(cv2.ml.SVM_C_SVC) # typ SVM ze stałą C odpowiadającą za dopasowanie hiperpłaszczyzny podziału\n",
    "mysvm.setKernel(cv2.ml.SVM_LINEAR) # typ funkcji jądrowej\n",
    "mysvm.setTermCriteria((cv2.TERM_CRITERIA_MAX_ITER, 10000, 1e-6)) # warunek zatrzymania uczenia klasyfikatora, tutaj liczba wykonanych iteracji\n",
    "mysvm.train(x_input, cv2.ml.ROW_SAMPLE, t_input) # właściwy trening/uczenie klasyfikatora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uD9USVSaxaUB"
   },
   "source": [
    "Więcej o parametrach SVM można dowiedzieć się tutaj: https://docs.opencv.org/2.4/modules/ml/doc/support_vector_machines.html#CvSVMParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rtA1sJp4lCcw",
    "outputId": "c41ca804-addf-44da-ccf1-3285cbd8ebb4"
   },
   "outputs": [],
   "source": [
    "# predykcja (określenie klasy) klasyfikatora dla zbioru uczącego w celu ewaluacji działania klasyfikatora\n",
    "y_pred = mysvm.predict(x_input)[1]\n",
    "svm_acc = accuracy_score(t_input, y_pred) #y_true = t_input\n",
    "print('SVM trained with '+str(svm_acc)+' accuracy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6L6ffl2l-IA",
    "outputId": "26a6da3c-6e07-4f45-80f4-4df06c997249"
   },
   "outputs": [],
   "source": [
    "# dokładniejszy raport dokładności klasyfikacji\n",
    "print(classification_report(t_input, y_pred, target_names=['ryz','soczewica','fasola'])) \n",
    "# kolumna support okresla ile próbek zawiera klasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "zue-HJjri_Hk",
    "outputId": "f3910584-d700-4d3d-acac-96a491df7bc3"
   },
   "outputs": [],
   "source": [
    "# macierz pomyłek (confusion matrix) - pozwala w przystępny sposób zwizualizować klasyfikację na zbiorze uczącym\n",
    "CMdisp = ConfusionMatrixDisplay(confusion_matrix(t_input, y_pred), display_labels=['ryz','soczewica','fasola'])\n",
    "CMdisp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyCZfnDQmuOP"
   },
   "source": [
    "Jak wydać na powyższym przykładzie, klasyfikator ma dość dobrą precyzję klasyfikacji, nie jest jednak nieomylny. Prawdopodobnie wynika to z mało licznego zbioru danych. Aby zwiększyć poprawność klasyfikacji można stosować wiele innych metod takich normalizacja wektora cech lub randomizacja kolejności próbek (to jednak już szczegóły uczenia maszynowego które na potrzeby tego przedmiotu nie są konieczne i nie będą omawiane). \n",
    "\n",
    "Zobaczmy jak klsayfikator radzi sobie z nowymi danymi, obrazem testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "J4u1KoTHnock",
    "outputId": "0a50c412-a09a-4714-82fd-22b51124409c"
   },
   "outputs": [],
   "source": [
    "img2 = cv2.imread('test1.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "feat_test = get_feat(img2)\n",
    "ret,thresh = cv2.threshold(img2,127,255,0)\n",
    "contours,hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE) \n",
    "\n",
    "img3 = cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB)\n",
    "for x in range(len(contours)):\n",
    "    sampleMat = np.float32(feat_test[:,x].reshape(-1,1).transpose())\n",
    "    response = mysvm.predict(sampleMat)[1]\n",
    "    cnt = contours[x]\n",
    "    if response == 1:\n",
    "      cv2.drawContours(img3, [cnt], 0, (0,255,0), 3) # zielony = ryz\n",
    "    elif response == 2:\n",
    "      cv2.drawContours(img3, [cnt], 0, (255,0,0), 3) # niebieski = soczewica\n",
    "    elif response == 3:\n",
    "      cv2.drawContours(img3, [cnt], 0, (0,0,255), 3) # czerwony = fasola\n",
    "    else:\n",
    "      cv2.drawContours(img3, [cnt], 0, (255,255,255), 3)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe-x8PB3o0fw"
   },
   "source": [
    "Jak widać obiekty klasy 'soczewica' zostały poprawnie sklasyfikowane, natomiast ziarno fasoli zostało sklasyfikowane jako klasa 'ryz'.\n",
    "Warto wspomnieć że obrazy tutaj używane zostały tak zmodyfikowane aby ułatwić segmentację a co za tym idzie pożniejszą klasyfikację. Poprawna segmentacja obiektów ma kluczowe znaczenie przy tworzeniu zbioru uczącego, szczególnie gdy mamy tak mało próbek jak w tym przypadku (~10 dla każdej z klas). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDldswer1Q81"
   },
   "source": [
    "## Wpływ przykładów uczących na wytrenowanie klasyfikatora\n",
    "\n",
    "Dokonując segmenacji obiektów, przed obliczeniem wektora cech, należy odszumić obraz, usunąć dziury z obiektu, rozdzielić obiety sklejone i wyeliminować artefakty.\n",
    "\n",
    "Poniżej przykłady wpływu zaburzeń na wyuczenie klasyfikatora. Dla łatwiejszego testowania wpływu jakości obrazów stanowiących zbiór uczący funkcja poniżej, przeprowadza uczenie klasyfikatora i prezentuje poziom wyuczenia w postaci raportu i 'confusion matrix'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExtANQ-5i1zn"
   },
   "outputs": [],
   "source": [
    "import svm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 764
    },
    "id": "7-sm0UXyrf8U",
    "outputId": "e514e405-3156-41b3-ed2c-e874a36198c1"
   },
   "outputs": [],
   "source": [
    "# wpływ braku wypełnienia dziur w obiektach na poprawność klasyfikacji\n",
    "img4 = cv2.imread('fasola2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "ret,thresh = cv2.threshold(img4,127,255,0)\n",
    "#cv2_imshow(thresh)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(thresh, cmap='gray')\n",
    "svm_3class = svm_utils.train_svm_3class ('train_ryz.jpg','train_soczewica.jpg','fasola2.jpg', cmplot=1, print_report=1, my_labels=['ryz','soczewica','fasola']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 971
    },
    "id": "mhYbVqGKrf0m",
    "outputId": "f0e07be8-7107-415c-90b7-edddba18728d"
   },
   "outputs": [],
   "source": [
    "# wpływ sklejonych obiektów na poprawność klasyfikacji\n",
    "img4 = cv2.imread('ryz1.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "ret,thresh = cv2.threshold(img4,127,255,0)\n",
    "#cv2_imshow(thresh)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(thresh, cmap='gray')\n",
    "svm_3class = svm_utils.train_svm_3class ('ryz1.jpg','train_soczewica.jpg','train_fasola.jpg', cmplot=1, print_report=1, my_labels=['ryz','soczewica','fasola']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lDOxcn3rj0q"
   },
   "source": [
    "## Wpływ preprocessingu na predykcję\n",
    "\n",
    "Nawet jeśli mamy poprawnie wyuczony klasyfikator, obraz który testujemy również trzeba przygotować (preprocessing). Poniżej wpływ zaszumienia i sklejonych obiektów na wynik klasyfikacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "uGBelunrbj4U",
    "outputId": "ac6f2503-98a7-4ded-e4ff-226f98044623"
   },
   "outputs": [],
   "source": [
    "# wracamy do poprawnego wytrenowania\n",
    "svm_3class = svm_utils.train_svm_3class ('train_ryz.jpg','train_soczewica.jpg','train_fasola.jpg', cmplot=1, print_report=1, my_labels=['ryz','soczewica','fasola']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "4egRa2Qktay4",
    "outputId": "f00b81ad-222e-456c-df5a-a14805acf5cc"
   },
   "outputs": [],
   "source": [
    "# wpływ zaszumienia na wynik klasyfikacji\n",
    "\n",
    "svm_utils.show_svm3_preds(svm_3class,'test1_noise.jpg') # funckcja prezentująca wynik dla podanego obrazu przy użyciu podanego wyuczonego klasyfikatora\n",
    "# zielony = ryz\n",
    "# niebieski = soczewica\n",
    "# czerwony = fasola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "JE_ElDl2pNae",
    "outputId": "fb8d0bb0-23f3-4eda-9cd7-a2af2a968b56"
   },
   "outputs": [],
   "source": [
    "# wpływ sklejonych obiektów na wynik klasyfikacji\n",
    "\n",
    "svm_utils.show_svm3_preds(svm_3class,'ryz1.jpg') # funckcja prezentująca wynik dla podanego obrazu przy użyciu podanego wyuczonego klasyfikatora\n",
    "# zielony = ryz\n",
    "# niebieski = soczewica\n",
    "# czerwony = fasola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAGkUm6mv5df"
   },
   "source": [
    "Podsumowując, SVM jest klasyfikatorem, który potrzebuje etykietowanych danych uczących. Na ich podstawie jest w stanie dokonać klasyfikacji z dość dobrą precyzją, niezbędne jest jednak zadbanie o jakość obrazu i poprawną segmentację."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcPQcmzKwHAR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "APO_Lab6_AnalizaObrazu.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
