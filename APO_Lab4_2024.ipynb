{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXAve7azp2Qy"
      },
      "source": [
        "# APO Lab 5 Segmentacja obrazu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9v3DebgjLgI"
      },
      "outputs": [],
      "source": [
        "#from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7AthW3AVYw4"
      },
      "source": [
        "!WAŻNE! Aby poniższy plik działał na Google Colab, należy wgrać plik 'lena_gray.bmp', '3shapes.bmp' oraz 'water_coins.jpg'. Z panelu po lewej stronie wybieramy ikonę folderu ('Files') a następnie 'Upload' i wybieramy zdjęcie (wcześniej ściągnięte na dysk twardy z UBI)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhiwTnFcLXMl"
      },
      "source": [
        "# Zadanie 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfYoPimSjPaX"
      },
      "outputs": [],
      "source": [
        "# Wczytanie obrazu pierwotnego\n",
        "img = cv2.imread('lena_gray.bmp', cv2.IMREAD_GRAYSCALE)\n",
        "#img = cv2.imread('page.png', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "#cv2_imshow(img)\n",
        "#plt.figure(figsize=(10,10))\n",
        "plt.imshow(img, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyLze_e9LPM5"
      },
      "source": [
        "## Zad 1. a) Segmentacja obrazu przez progowanie z ręcznie wyznaczonym progiem (thresholding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KZ1H6iAIeix"
      },
      "source": [
        "\n",
        "Algorytm progowania realizuje prostą zależność gdzie wartości powyżej ustalonego progu przyjmują wartośc 1 (lub 255) a wartości poniżej wartość 0.\n",
        "\n",
        "![alt text](https://github.com/knave88/APO/raw/main/progowanie.png)\n",
        "\n",
        "W efekcie otrzymujemy binarny obraz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TanGLU-RmSvG"
      },
      "outputs": [],
      "source": [
        "# ręczne ustawienie progu\n",
        "myThresh = 127\n",
        "# progowanie z powyższyą wartością progu\n",
        "ret,img_th_binary = cv2.threshold(img,myThresh,255,cv2.THRESH_BINARY)\n",
        "\n",
        "#cv2_imshow(img_th_binary)\n",
        "#plt.figure(figsize=(10,10))\n",
        "plt.imshow(img_th_binary, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6a-n_NBgZ-R"
      },
      "source": [
        "W programie zaliczenowym należy dać możliwość użytkownikowi interaktywnego doboru progu (najlepiej z aktywnym podglądem, czyli odświeżaniem obrazu po zmianie wartości progu). Przyład implementacji poniżej z zastosowaniem suwaka (slider) do przyjęcia wartości progu od użytkowanika."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYkMw2DEfhg7"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def fun_thresh(slider_threshold_value):\n",
        "    print(\"Wybrana wastość progu: \" + str(slider_threshold_value))\n",
        "\n",
        "    # progowanie z powyższyą wartością progu\n",
        "    ret,img_th_binary2 = cv2.threshold(img,slider_threshold_value,255,cv2.THRESH_BINARY)\n",
        "\n",
        "    #cv2_imshow(img_th_binary2)\n",
        "    #plt.figure(figsize=(10,10))\n",
        "    plt.imshow(cv2.cvtColor(img_th_binary2, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    return slider_threshold_value\n",
        "\n",
        "#interact(fun_thresh, slider_threshold_value=58); # widgets.IntSlider(min=-10, max=30, step=1, value=10)\n",
        "interact(fun_thresh, slider_threshold_value=widgets.IntSlider(min=0, max=255, step=1, value=127));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbKaaK_aLjDY"
      },
      "source": [
        "## Zad 1. b) Segmentacja obrazu przez progowanie adaptacyjne (adaptive thresholding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfUYyyLqMEZR"
      },
      "source": [
        "Funkcja progowania adaptacyjnego wykonuje lokalne progowanie, dla każdego piksela jest wyznaczany próg na podstawie obszaru otaczającego analizowany piksel, zgodnie ze schematem okna przesuwnego.\n",
        "\n",
        "W bibliotece openCV aby użyć funkcji cv2.adaptiveThreshold należy podać jako parametry wejściowe:\n",
        "*   obraz wejściowy (img)\n",
        "*   maksymalną wartość dla obrazu (255)\n",
        "*   sposób liczenia progu (adaptive_method = cv2.ADAPTIVE_THRESH_MEAN_C) na podstawie wartości średniej, lub (ADAPTIVE_THRESH_GAUSSIAN_C) średniej ważonej\n",
        "*   progowanie wprost (threshold_type = cv2.THRESH_BINARY), lub (THRESH_BINARY_INV) z inwersją\n",
        "*   rozmiar okna poddawanego analizie, powinno mieć wartość nieprzystą (block_size = 11)\n",
        "*   stała wartość odejmowana od średniej (5)\n",
        "\n",
        "Poniżej przedstawiono dwie wersje liczenia progu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKmwB_xrGi7R"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_value = 255\n",
        "adaptive_method = cv2.ADAPTIVE_THRESH_MEAN_C\n",
        "threshold_type = cv2.THRESH_BINARY\n",
        "block_size = 11\n",
        "\n",
        "# progowanie z powyższyą wartością progu\n",
        "img_th_adapt = cv2.adaptiveThreshold(img, max_value, adaptive_method, threshold_type, block_size, 5)\n",
        "\n",
        "#cv2_imshow(img_th_binary)\n",
        "frame = cv2.hconcat((img, img_th_adapt))\n",
        "#plt.figure(figsize=(10,10))\n",
        "plt.imshow(frame, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRgeQ9KvthWJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_value = 255\n",
        "adaptive_method = cv2.ADAPTIVE_THRESH_GAUSSIAN_C # średniej ważonej\n",
        "threshold_type = cv2.THRESH_BINARY\n",
        "block_size = 11\n",
        "\n",
        "# progowanie z powyższyą wartością progu\n",
        "img_th_adapt = cv2.adaptiveThreshold(img, max_value, adaptive_method, threshold_type, block_size, 5)\n",
        "\n",
        "#cv2_imshow(img_th_binary)\n",
        "frame = cv2.hconcat((img, img_th_adapt))\n",
        "#plt.figure(figsize=(10,10))\n",
        "plt.imshow(frame, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlICRcyWjSzv"
      },
      "source": [
        "W tym przypadku od użytkownika należy przyjąć wartość rozmiaru okna przetwarania. Uwaga, należy pamiętać, że rozmiar okna przetwarzania powinien być liczbą nieparzystą większą lub równą 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhWtxsQrfhg8"
      },
      "outputs": [],
      "source": [
        "def fun_thresh_adapt(slider_threshold_window):\n",
        "    print(\"Wybrana wielkosc okna: \" + str(slider_threshold_window))\n",
        "\n",
        "    max_value = 255\n",
        "    adaptive_method = cv2.ADAPTIVE_THRESH_MEAN_C\n",
        "    threshold_type = cv2.THRESH_BINARY\n",
        "    block_size = slider_threshold_window         # wartosc podawana intraktywnie\n",
        "\n",
        "    # progowanie z powyższyą wartością progu\n",
        "    img_th_adapt = cv2.adaptiveThreshold(img, max_value, adaptive_method, threshold_type, block_size, 5)\n",
        "\n",
        "    #cv2_imshow(img_th_adapt)\n",
        "    frame = cv2.hconcat((img, img_th_adapt))\n",
        "    #cv2_imshow(frame)\n",
        "    plt.imshow(frame, cmap='gray')\n",
        "\n",
        "    return slider_threshold_window\n",
        "\n",
        "interact(fun_thresh_adapt, slider_threshold_window=widgets.IntSlider(min=0, max=255, step=1, value=21));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6W6QgZenwVp"
      },
      "source": [
        "## Zad 1. c) Segmentacja obrazu przez progowanie metodą Otsu (Otsu thresholding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET-0OGzfbdnp"
      },
      "source": [
        "Jest to metoda progowania, oparta na histogramie. Metoda polega na minimalizacji sumy ważonej wariancji dwóch klas (tła i obiektów pierwszego planu), co jest tożsame z maksymalizacją wariancji międzyklasowej. Metodę tę można rozszerzyć do metody wieloprogowej jak również do progowania lokalnego.\n",
        "\n",
        "Często stosuje się najpierw lekkie rozmycie obrazu a dopiero potem progowanie Otsu. Pozwala to m.in. na uniknięcie małych artefaktów.\n",
        "\n",
        "Publikacja: Nobuyuki Otsu, A Threshold Selection Method from Gray-Level Histograms, „IEEE Transactions on Systems, Man, and Cybernetics”, 9 (1), 1979, s. 62–66, DOI: 10.1109/TSMC.1979.4310076"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGzFlRsIbeLi"
      },
      "outputs": [],
      "source": [
        "# Otsu's thresholding\n",
        "ret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "\n",
        "# Otsu's thresholding after Gaussian filtering\n",
        "blur = cv2.GaussianBlur(img,(5,5),0)\n",
        "ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "\n",
        "frame = cv2.hconcat((th2, th3))\n",
        "#cv2_imshow(frame)\n",
        "plt.imshow(frame, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 2\n",
        "## Segmentacja metodą GrabCut"
      ],
      "metadata": {
        "id": "9bL-QaYtf4I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('lena.bmp', cv2.IMREAD_COLOR)\n",
        "\n",
        "\n",
        "mask = np.zeros(img.shape[:2], np.uint8)\n",
        "\n",
        "\n",
        "backgroundModel = np.zeros((1, 65), np.float64)\n",
        "foregroundModel = np.zeros((1, 65), np.float64)\n",
        "\n",
        "\n",
        "rectangle = (45, 80, 350, 450) # docelowo to pobieramy od uzytkownika\n",
        "\n",
        "\n",
        "cv2.grabCut(img, mask, rectangle,\n",
        "            backgroundModel, foregroundModel,\n",
        "            3, cv2.GC_INIT_WITH_RECT)\n",
        "\n",
        "\n",
        "mask2 = np.where((mask == 2)|(mask == 0), 0, 1).astype('uint8')\n",
        "\n",
        "image_segmented = img * mask2[:, :, np.newaxis]\n",
        "\n",
        "plt.imshow(cv2.cvtColor(image_segmented, cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "sCsxe-Qvf-zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6nW3jPWb7S6"
      },
      "source": [
        "# Zadanie 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiuAvOYkeItF"
      },
      "source": [
        "## Segmentacja obrazu metodą wododziałową (watershed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re4u-kS4cDdu"
      },
      "source": [
        "Dział wodny (wododział, watershed) to granica pomiędzy zlewiskami różnych rzek\n",
        "lub zbiorników wodnych. Wododziały są zawsze umiejscowione na grzbietach funkcji wysokości terenu.\n",
        "W analizie i przetwarzaniu obrazów pod pojęciem działu wodnego rozumie się operację morfologiczną na obszarze, którego centrum stanowi lokalne minimum w obrazie. Obszary o małej intensywności stanowią w obrazie lokalne „doliny”, zaś obszary o dużej intensywności – lokalne „wzniesienia” . Jako wysokość terenu przyjmuje się amplitudę gradientu intensywności pikseli lub samą\n",
        "intensywność, za wododział zaś – grzbiety tych funkcji.\n",
        "\n",
        "Użycie metody watershed wymaga najpierw wyznaczenia znaczników obszarów które jednoznacznie należą do tła oraz takich które jednoznacznie należą do obiektów. Z pomocą tych obrazów jest wykonywane rozdzielenie obiektów sklejonych, co przedstawiono poniżej."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJa44sHuw6qP"
      },
      "outputs": [],
      "source": [
        "# Wczytanie obrazu do segmentacji\n",
        "img2 = cv2.imread('water_coins.jpg', cv2.IMREAD_COLOR) # water_coins.jpg\n",
        "#cv2_imshow(img2)\n",
        "plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHtsXYnNbMfQ"
      },
      "outputs": [],
      "source": [
        "# sprowadzenie obrazu do szaroodcieniowego (grayscale)\n",
        "img_gray = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
        "# progowanie obrazu czyli wstępne wyznaczenie obiektów\n",
        "ret2,thresh = cv2.threshold(img_gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "\n",
        "#cv2_imshow(thresh)\n",
        "plt.imshow(thresh, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63194QHtvpcg"
      },
      "outputs": [],
      "source": [
        "# odszumianie obrazu przez operacje morfologiczne\n",
        "kernel = np.ones((3,3),np.uint8)\n",
        "opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 1)\n",
        "\n",
        "#cv2_imshow(opening)\n",
        "plt.imshow(opening, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjN0AvfOvpTc"
      },
      "outputs": [],
      "source": [
        "# wyznaczenie jednoznacznych obszarów tła\n",
        "sure_bg = cv2.dilate(opening,kernel,iterations=1)\n",
        "\n",
        "#cv2_imshow(sure_bg)\n",
        "plt.imshow(sure_bg, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDneSuM2yA6e"
      },
      "outputs": [],
      "source": [
        "# wyznaczenie jednoznacznych obszarów obiektów\n",
        "\n",
        "# krok pośredni: transformata odległościowa\n",
        "dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
        "print (\"Obraz transformaty odległościwej (pseudokolor dla lepszej wizualizacji)\")\n",
        "#cv2_imshow(cv2.applyColorMap(np.uint8(dist_transform*10), cv2.COLORMAP_JET))\n",
        "plt.imshow(cv2.applyColorMap(np.uint8(dist_transform*10), cv2.COLORMAP_JET))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuDxIrJzfhg9"
      },
      "outputs": [],
      "source": [
        "# określenie jednoznacznych obszarów obiektów przez progowanie obrazu transformaty odlegościowej\n",
        "print (\"Obraz jednoznacznych obszarów należących do obiektów\")\n",
        "ret, sure_fg = cv2.threshold(dist_transform,0.5*dist_transform.max(),255,0)\n",
        "sure_fg = np.uint8(sure_fg)\n",
        "#cv2_imshow(sure_fg)\n",
        "plt.imshow(sure_fg, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl6pfFekvpKq"
      },
      "outputs": [],
      "source": [
        "# wyznaczenie obszarów \"niepewnych\"\n",
        "# takich w których znajdują się krawędzie obiektu\n",
        "# obraz uzyskujemy przez odjęcie obszarów jednoznacznych\n",
        "unknown = cv2.subtract(sure_bg,sure_fg)\n",
        "\n",
        "#cv2_imshow(unknown)\n",
        "plt.imshow(unknown, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMKu62y3vo_v"
      },
      "outputs": [],
      "source": [
        "# etykietowanie obiektów\n",
        "ret, markers = cv2.connectedComponents(sure_fg)\n",
        "print(\"Znalezionych obiektów:  \"+str(np.max(markers)))\n",
        "#cv2_imshow(cv2.applyColorMap(np.uint8(markers*10), cv2.COLORMAP_JET))\n",
        "plt.imshow(cv2.applyColorMap(np.uint8(markers*10), cv2.COLORMAP_JET))\n",
        "#(pseudokolor dla lepszej wizualizacji)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btdzHE0pwSZX"
      },
      "outputs": [],
      "source": [
        "# dodanie wartości 1 do etykiet, tak aby tło miało wartość 1 a nie 0.\n",
        "markers = markers+1\n",
        "\n",
        "# oznaczenie obszarów \"niepewnych\" jako zero\n",
        "markers[unknown==255] = 0\n",
        "\n",
        "#cv2_imshow(markers)\n",
        "#cv2_imshow(cv2.applyColorMap(np.uint8(markers*10), cv2.COLORMAP_JET)) #(pseudokolor dla lepszej wizualizacji)\n",
        "plt.imshow(cv2.applyColorMap(np.uint8(markers*10), cv2.COLORMAP_JET))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFAlwz-ZwSSE"
      },
      "outputs": [],
      "source": [
        "# aplikacj algorytmu wododziału (watershed) do orygnalnego kolorowego obrazu (img2),\n",
        "# dodatkowy argument wymagany do wykonania tej operacji to obliczony powyżej obraz etykietowany\n",
        "markers2 = cv2.watershed(img2, markers)\n",
        "\n",
        "#cv2_imshow(cv2.applyColorMap(np.uint8(markers2*10), cv2.COLORMAP_JET)) #(pseudokolor dla lepszej wizualizacji)\n",
        "plt.imshow(cv2.applyColorMap(np.uint8(markers2*10), cv2.COLORMAP_JET))\n",
        "\n",
        "print(\"Znaleziono \"+ str(np.max(markers2)) + \" obiektów.\")\n",
        "\n",
        "# Wynik:\n",
        "# Granice obiektów zaznaczone w obrazie wartością -1\n",
        "# Obiekty etykietowane kolejnymi wartościami"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3iICZHDo1xj"
      },
      "outputs": [],
      "source": [
        "# wstawienie linii krawędzi obiektów do obrazu szaroodcieniowego\n",
        "img_gray[markers2 == -1] = 255\n",
        "\n",
        "#cv2_imshow(img_gray)\n",
        "plt.imshow(img_gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WWgUqZgwSLT"
      },
      "outputs": [],
      "source": [
        "# wstawienie linii krawędzi obiektów do oryginalnego obrazu kolorowego\n",
        "img2[markers2 == -1] = [255,0,0]\n",
        "\n",
        "#cv2_imshow(img2)\n",
        "plt.imshow(img2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S04mwY2yfhg9"
      },
      "outputs": [],
      "source": [
        "# wizualzacja obiektow (labelling)\n",
        "markers2_colored = cv2.applyColorMap(np.uint8(markers2*10), cv2.COLORMAP_JET)\n",
        "img_blend = cv2.addWeighted(np.stack((img_gray,)*3, axis=-1),0.7,markers2_colored,0.5,-1)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(img_blend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU6PBZBDf6FA"
      },
      "source": [
        "Wynik algorytmu watershed zawarty jest w zmiennej 'markers2'. Jest to obraz odpowiadający rozmiarem obrazowi orygnialnemu w którym oddzielne obiekty są etykietowane (każdy obiekt ma przypisaną jedną wartość) a granice obiektów zaznaczone są wartością -1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuOXZezvqka9"
      },
      "outputs": [],
      "source": [
        "# powyższa procedura zamknięta w jednej funkcji\n",
        "def my_watershed(image):\n",
        "  img_gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  ret2,thresh = cv2.threshold(img_gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "  kernel = np.ones((3,3),np.uint8)\n",
        "  opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 1)\n",
        "\n",
        "  sure_bg = cv2.dilate(opening,kernel,iterations=1)\n",
        "  dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
        "\n",
        "  ret, sure_fg = cv2.threshold(dist_transform,0.5*dist_transform.max(),255,0)\n",
        "  sure_fg = np.uint8(sure_fg)\n",
        "\n",
        "  unknown = cv2.subtract(sure_bg,sure_fg)\n",
        "  ret, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "  markers = markers+1\n",
        "  markers[unknown==255] = 0\n",
        "\n",
        "  markers2 = cv2.watershed(image, markers)\n",
        "\n",
        "  img_gray[markers2 == -1] = 255\n",
        "  image[markers2 == -1] = [255,0,0]\n",
        "\n",
        "  print(\"Znaleziono \"+ str(np.max(markers2)) + \" obiektów.\")\n",
        "\n",
        "  # wizuaizacja: obraz oryginalny oraz wynik algorytmy watershed\n",
        "  frame = cv2.hconcat(( image, cv2.applyColorMap(np.uint8(markers2*10), cv2.COLORMAP_JET) ))\n",
        "  #cv2_imshow(frame)\n",
        "  plt.imshow(frame)\n",
        "\n",
        "  return markers2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_v7MuIjkWkW"
      },
      "outputs": [],
      "source": [
        "# Dodatkowe przykłady\n",
        "out1 = my_watershed(cv2.imread('soczewica2.jpg', cv2.IMREAD_COLOR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU3z6Z1Hfhg-"
      },
      "outputs": [],
      "source": [
        "out2 = my_watershed(cv2.imread('ryz1.jpg', cv2.IMREAD_COLOR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQTN1SmTpe8d"
      },
      "source": [
        "Jak widać na powyższych przykładach algorytm watershed nie jest idealny. Obiekty które nachodzą na siebie w większym stopniu niż stykanie się są trudne do rozdzielenia tą metodą. Ponadto, klastry zawierające dużą liczbę obietków również stanowią problem. Algorytm mylnie rozpoznaje granice lub nie rozdziela obiektów na pojedyncze."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 4\n",
        "## Metoda Inpainting"
      ],
      "metadata": {
        "id": "1Sj5B7HBgFf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('lena_vandalized.bmp')\n",
        "plt.imshow(img)\n"
      ],
      "metadata": {
        "id": "aE6nU6jLgFs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = cv2.imread('lena_mask3.bmp', 0)\n",
        "plt.imshow(mask)"
      ],
      "metadata": {
        "id": "jlhrh5fFl9WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inpaint\n",
        "img_inpainted = cv2.inpaint(img, mask, 3, cv2.INPAINT_TELEA) # alternatywa: cv2.INPAINT_TELEA lub cv2.INPAINT_NS\n",
        "\n",
        "frame = cv2.hconcat((img, img_inpainted))\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(frame)"
      ],
      "metadata": {
        "id": "U1uvE06Pl1Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 5\n",
        "## Kompresja metodą RLE"
      ],
      "metadata": {
        "id": "G5P108UggqTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Algorytm kompresji realizuje prostą formułę gdzie kodowane są ciągi identycznych symboli. Para zawiera symbol i liczbę jego powtórzeń.\n",
        "\n",
        "![alt text](https://github.com/knave88/APO/raw/main/rle.jpg)\n"
      ],
      "metadata": {
        "id": "baymLUtOg0gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RLE_string_encode(input_string):\n",
        "    count = 1\n",
        "    prev = ''\n",
        "    lst = []\n",
        "    for character in input_string:\n",
        "        if character != prev:\n",
        "            if prev:\n",
        "                entry = (prev,count)\n",
        "                lst.append(entry)\n",
        "                #print lst\n",
        "            count = 1\n",
        "            prev = character\n",
        "        else:\n",
        "            count += 1\n",
        "    else:\n",
        "        try:\n",
        "            entry = (character,count)\n",
        "            lst.append(entry)\n",
        "            return (lst, 0)\n",
        "        except Exception as e:\n",
        "            print(\"Exception encountered {e}\".format(e=e))\n",
        "            return (e, 1)\n"
      ],
      "metadata": {
        "id": "NIQQ5-PDgz23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RLE_string_decode(lst):\n",
        "    q = \"\"\n",
        "    for character, count in lst:\n",
        "        q += character * count\n",
        "    return q"
      ],
      "metadata": {
        "id": "9_7uUX6sgqma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value = RLE_string_encode(\"aaaaapppppppppoooooooooooooo\")\n",
        "if value[1] == 0:\n",
        "    print(\"Encoded value is {}\".format(value[0]))\n",
        "    RLE_string_decode(value[0])"
      ],
      "metadata": {
        "id": "2ifXdsOKg7aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 6\n",
        "## Analiza obiektów"
      ],
      "metadata": {
        "id": "mJ6z9py3hDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wczytanie obrazu pierwotnego\n",
        "img = cv2.imread('3shapes.bmp', cv2.IMREAD_GRAYSCALE)\n",
        "#cv2_imshow(img)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "id": "XXFymbOChF09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L7oo6l5FRbT"
      },
      "source": [
        "Funkcja cv2.findContours pozwala w prosty sposób wybrać z obrazu binarnego obiekty a właściwie ich kontury/obrysy. Jej ierwszym argumentem wejściowym jest oczywiście obraz. Dwa kolejne parametry definiują szczegóły generowania konturów. Drugi parametr określa czy znalezione zostaną wyłącznie kontóry zewnętrzne czy wszystie i określa hierarchiczność. W tym przykładzie będziemy korzystać z opcji *cv2.RETR_LIST*, która generuje wszystkie kontóry w obrazie bez ustalania ich hierarchii. Trzeci paramer określa czy i w jaki sposób kontury obiektów będą przybliżane (aproksymowane) aby zminimalizować liczbę punktów niezbędnych do przechowania. W tym przykładzie będziemy korzystać z opcji *cv2.CHAIN_APPROX_SIMPLE*, która enkoduje linie poziome, pionowe i nachylone pod kątem 45 stopni.\n",
        "\n",
        "Pozostałe opcje drugiego parametru:\n",
        "\n",
        "\n",
        "*   RETR_EXTERNAL\n",
        "Python: cv.RETR_EXTERNAL\n",
        "retrieves only the extreme outer contours. It sets hierarchy[i][2]=hierarchy[i][3]=-1 for all the contours.\n",
        "\n",
        "*   RETR_LIST\n",
        "Python: cv.RETR_LIST\n",
        "retrieves all of the contours without establishing any hierarchical relationships.\n",
        "\n",
        "*   RETR_CCOMP\n",
        "Python: cv.RETR_CCOMP\n",
        "retrieves all of the contours and organizes them into a two-level hierarchy. At the top level, there are external boundaries of the components. At the second level, there are boundaries of the holes. If there is another contour inside a hole of a connected component, it is still put at the top level.\n",
        "\n",
        "*   RETR_TREE\n",
        "Python: cv.RETR_TREE\n",
        "retrieves all of the contours and reconstructs a full hierarchy of nested contours.\n",
        "\n",
        "*   RETR_FLOODFILL\n",
        "Python: cv.RETR_FLOODFILL\n",
        "\n",
        "Pozostałe opcje trzeciego parametru:\n",
        "\n",
        "*   CHAIN_APPROX_NONE\n",
        "Python: cv.CHAIN_APPROX_NONE\n",
        "stores absolutely all the contour points. That is, any 2 subsequent points (x1,y1) and (x2,y2) of the contour will be either horizontal, vertical or diagonal neighbors, that is, max(abs(x1-x2),abs(y2-y1))==1.\n",
        "\n",
        "*   CHAIN_APPROX_SIMPLE\n",
        "Python: cv.CHAIN_APPROX_SIMPLE\n",
        "compresses horizontal, vertical, and diagonal segments and leaves only their end points. For example, an up-right rectangular contour is encoded with 4 points.\n",
        "\n",
        "*   CHAIN_APPROX_TC89_L1\n",
        "Python: cv.CHAIN_APPROX_TC89_L1\n",
        "applies one of the flavors of the Teh-Chin chain approximation algorithm\n",
        "\n",
        "*   CHAIN_APPROX_TC89_KCOS\n",
        "Python: cv.CHAIN_APPROX_TC89_KCOS\n",
        "applies one of the flavors of the Teh-Chin chain approximation algorithm\n",
        "\n",
        "\n",
        "[Referencja] Dokumentacja: https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#findcontours"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preporcessing\n",
        "ret,thresh = cv2.threshold(img,127,255,0) # pogowanie obrazu pierwotnego aby uzyskac obraz binarny\n",
        "contours,hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE) # funkcja znajdywania kontórow w obrazie binarnym\n",
        "len(contours) # liczb znalezionych konturów"
      ],
      "metadata": {
        "id": "cgpOXVdehQ7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siKhgP2OJntX"
      },
      "source": [
        "Jak łatwo zauważyć pomimo że na obrazie są trzy obiekty to algorytm znalazł 4 kontury. Zobaczmy dlaczego. Narysyjemy wszystkie znalezione kontury na obrazie."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rysowanie kontórów na obrazie\n",
        "\n",
        "img2 = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB) # konwersja z szaroodcieniowego do RGB (właściwie to kolejność BGR) aby można było rysować kolorowe kontury\n",
        "\n",
        "# rysujemy pierwszy kontur kolorem niebieskim\n",
        "cnt = contours[0]\n",
        "cv2.drawContours(img2, [cnt], 0, (255,0,0), 3)\n",
        "\n",
        "# rysujemy drugi kontur kolorem zielonym\n",
        "cnt = contours[1]\n",
        "cv2.drawContours(img2, [cnt], 0, (0,255,0), 3)\n",
        "\n",
        "# rysujemy trzeci kontur kolorem czerwonym\n",
        "cnt = contours[2]\n",
        "cv2.drawContours(img2, [cnt], 0, (0,0,255), 3)\n",
        "\n",
        "# rysujemy czwarty kontur innym kolorem\n",
        "cnt = contours[3]\n",
        "cv2.drawContours(img2, [cnt], 0, (255,255,0), 3)\n",
        "\n",
        "#cv2_imshow(img2)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "4dZ25oDrhT-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNt-W6joKQJ5"
      },
      "source": [
        "Jak widać algorytm znajduje również kontur najbardziej zewnętrzny, czyli ramkę obrazu. Poniżej definicja funkcji która wczytuje obraz i wyświetla naniesione kolorowe kontury na obraz po progowaniu. Zachęcam do przetestowania różnych kombinacji parametrów funkcji *cv2.findContours*."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_and_show_contours (fname):\n",
        "  img = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n",
        "  ret,thresh = cv2.threshold(img,127,255,0)\n",
        "  contours,hierarchy = cv2.findContours(thresh, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n",
        "# mode: cv2.RETR_EXTERNAL / cv2.RETR_FLOODFILL\n",
        "# approximation: cv2.CHAIN_APPROX_NONE / cv2.CHAIN_APPROX_SIMPLE\n",
        "  img3 = cv2.cvtColor(thresh,cv2.COLOR_GRAY2RGB)\n",
        "  for cnt in contours:\n",
        "    cv2.drawContours(img3, [cnt], 0, (random.randrange(50,200,25),random.randrange(50,200,25),random.randrange(50,200,25)), 3)\n",
        "  #cv2_imshow(img3)\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.imshow(img3)"
      ],
      "metadata": {
        "id": "CFCSUxYYhWcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_and_show_contours ('train_fasola.jpg')"
      ],
      "metadata": {
        "id": "oHSP79VShXnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEeO-wu7heLA"
      },
      "source": [
        "## Zad 1. a) Obliczenie momentów\n",
        "\n",
        "Do obliczania momentów centralnych obrazu (obiektu) służy wbudowana funkcja opencv *cv2.moments*. Może ona przyjmować jako argument wejściowy cały obraz lub kontur (uzyskany z poprzednio użytej funkcji *cv2.findContours*).\n",
        "\n",
        "Funkcja ta wylicza momenty do 3 rzędu wiec w wyniku otrzymujemy 24 wartości. W języku python wynik jest w postaci 'dict'.\n",
        "\n",
        "[Referencja] Dokumentacja: https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=cv2.moments#cv2.moments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# obliczanie momentów dla całego obrazu\n",
        "M_img = cv2.moments(img)\n",
        "M_img"
      ],
      "metadata": {
        "id": "4SdJLLcSheo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obliczanie momentów dla pojedynczego obiektu\n",
        "cnt = contours[0]\n",
        "M = cv2.moments(cnt)\n",
        "M"
      ],
      "metadata": {
        "id": "7rIzNQNmhf14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXQJc0qehhxB"
      },
      "source": [
        "## Zad 1. b) Obliczenie pola powierzchni i obwodu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "area = cv2.contourArea(cnt)\n",
        "area"
      ],
      "metadata": {
        "id": "3Y3YHWDkhiyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perimeter = cv2.arcLength(cnt,True) # Drugi argument określa czy kształt ma zamknięty kontur (wtedy podajemy True) czy jest to krzywa\n",
        "perimeter"
      ],
      "metadata": {
        "id": "MWOlGEkOhjM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3BO3dIxhlKH"
      },
      "source": [
        "## Zad 1. c) Obliczanie współczynników kształtu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#aspect ratio\n",
        "x,y,w,h = cv2.boundingRect(cnt)\n",
        "aspect_ratio = float(w)/h\n",
        "aspect_ratio"
      ],
      "metadata": {
        "id": "U5WQENlZhmDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extent\n",
        "area = cv2.contourArea(cnt)\n",
        "x,y,w,h = cv2.boundingRect(cnt)\n",
        "rect_area = w*h\n",
        "extent = float(area)/rect_area\n",
        "extent"
      ],
      "metadata": {
        "id": "VQBPsTI8hmfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#solidity\n",
        "area = cv2.contourArea(cnt)\n",
        "hull = cv2.convexHull(cnt)\n",
        "hull_area = cv2.contourArea(hull)\n",
        "solidity = float(area)/hull_area\n",
        "solidity"
      ],
      "metadata": {
        "id": "Nksf1J4rhnmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# equivalentDiameter\n",
        "area = cv2.contourArea(cnt)\n",
        "equi_diameter = np.sqrt(4*area/np.pi)\n",
        "equi_diameter"
      ],
      "metadata": {
        "id": "J8F3t9ndhoaF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}